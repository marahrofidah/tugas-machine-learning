{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":109264,"sourceType":"datasetVersion","datasetId":56828}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\n\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom torchvision.transforms import transforms\n\nimport yaml\n\nimport logging\nimport sys\nimport csv\nimport torch\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\n\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntorch.manual_seed(0)\nnp.random.seed(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"\nclass GaussianBlur(object):\n    \"\"\"blur a single image on CPU\"\"\"\n    def __init__(self, kernel_size):\n        radias = kernel_size // 2\n        kernel_size = radias * 2 + 1\n        self.blur_h = nn.Conv2d(3, 3, kernel_size=(kernel_size, 1),\n                                stride=1, padding=0, bias=False, groups=3)\n        self.blur_v = nn.Conv2d(3, 3, kernel_size=(1, kernel_size),\n                                stride=1, padding=0, bias=False, groups=3)\n        self.k = kernel_size\n        self.r = radias\n\n        self.blur = nn.Sequential(\n            nn.ReflectionPad2d(radias),\n            self.blur_h,\n            self.blur_v\n        )\n\n        self.pil_to_tensor = transforms.ToTensor()\n        self.tensor_to_pil = transforms.ToPILImage()\n\n    def __call__(self, img):\n        img = self.pil_to_tensor(img).unsqueeze(0)\n\n        sigma = np.random.uniform(0.1, 2.0)\n        x = np.arange(-self.r, self.r + 1)\n        x = np.exp(-np.power(x, 2) / (2 * sigma * sigma))\n        x = x / x.sum()\n        x = torch.from_numpy(x).view(1, -1).repeat(3, 1)\n\n        self.blur_h.weight.data.copy_(x.view(3, 1, self.k, 1))\n        self.blur_v.weight.data.copy_(x.view(3, 1, 1, self.k))\n\n        with torch.no_grad():\n            img = self.blur(img)\n            img = img.squeeze()\n\n        img = self.tensor_to_pil(img)\n\n        return img\n        \ndef save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, 'model_best.pth.tar')\n\n\ndef save_config_file(model_checkpoints_folder, args):\n    if not os.path.exists(model_checkpoints_folder):\n        os.makedirs(model_checkpoints_folder)\n        with open(os.path.join(model_checkpoints_folder, 'config.yml'), 'w') as outfile:\n            yaml.dump(args, outfile, default_flow_style=False)\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SimCLR","metadata":{}},{"cell_type":"code","source":"\nclass SimCLR(object):\n\n    def __init__(self, *args, **kwargs):\n        self.args = kwargs['args']\n        self.model = kwargs['model'].to(self.args.device)\n        self.optimizer = kwargs['optimizer']\n        self.scheduler = kwargs['scheduler']\n        # self.writer = SummaryWriter()\n        # logging.basicConfig(filename=os.path.join(self.writer.log_dir, 'training.log'), level=logging.DEBUG)\n        self.criterion = torch.nn.CrossEntropyLoss().to(self.args.device)\n\n        os.makedirs(self.args.log_dir, exist_ok=True)\n        logging.basicConfig(filename=os.path.join(self.args.log_dir, 'training.log'),\n                            level=logging.INFO,\n                            format='%(asctime)s - %(levelname)s - %(message)s')\n\n        self.csv_log_path = os.path.join(self.args.log_dir, 'metrics.csv')\n        with open(self.csv_log_path, mode='w', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerow(['step', 'loss', 'acc_top1', 'acc_top5', 'learning_rate'])\n\n        \n\n    def info_nce_loss(self, features):\n\n        labels = torch.cat([torch.arange(self.args.batch_size) for i in range(self.args.n_views)], dim=0)\n        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n        labels = labels.to(self.args.device)\n\n        features = F.normalize(features, dim=1)\n\n        similarity_matrix = torch.matmul(features, features.T)\n        # assert similarity_matrix.shape == (\n        #     self.args.n_views * self.args.batch_size, self.args.n_views * self.args.batch_size)\n        # assert similarity_matrix.shape == labels.shape\n\n        # discard the main diagonal from both: labels and similarities matrix\n        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.args.device)\n        labels = labels[~mask].view(labels.shape[0], -1)\n        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n        # assert similarity_matrix.shape == labels.shape\n\n        # select and combine multiple positives\n        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n\n        # select only the negatives the negatives\n        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)\n\n        logits = torch.cat([positives, negatives], dim=1)\n        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.args.device)\n\n        logits = logits / self.args.temperature\n        return logits, labels\n\n    def train(self, train_loader):\n\n        scaler = GradScaler(enabled=self.args.fp16_precision)\n\n        # save config file\n        # save_config_file(self.writer.log_dir, self.args)\n\n        n_iter = 0\n        best_top1 = 0.0  \n        logging.info(f\"Start SimCLR training for {self.args.epochs} epochs.\")\n\n        for epoch_counter in range(self.args.epochs):\n            for images, _ in tqdm(train_loader):\n                images = torch.cat(images, dim=0)\n\n                images = images.to(self.args.device)\n\n                with autocast(enabled=self.args.fp16_precision):\n                    features = self.model(images)\n                    logits, labels = self.info_nce_loss(features)\n                    loss = self.criterion(logits, labels)\n\n                self.optimizer.zero_grad()\n\n                scaler.scale(loss).backward()\n\n                scaler.step(self.optimizer)\n                scaler.update()\n\n                if n_iter % self.args.log_every_n_steps == 0:\n                    top1, top5 = accuracy(logits, labels, topk=(1, 5))\n                    lr = self.scheduler.get_last_lr()[0]\n\n                    print(f\"[Epoch {epoch_counter+1}] Step {n_iter}: Loss={loss:.4f}, Top1={top1[0]:.2f}%, Top5={top5[0]:.2f}%, LR={lr:.6f}\")\n\n\n                    # File log\n                    with open(self.csv_log_path, mode='a', newline='') as f:\n                        writer = csv.writer(f)\n                        writer.writerow([n_iter, loss.item(), top1[0].item(), top5[0].item(), lr])\n\n                    if top1[0].item() > best_top1:\n                        best_top1 = top1[0].item()\n                        best_ckpt_path = os.path.join(self.args.log_dir, 'checkpoint_best.pth')\n                        save_checkpoint({\n                            'epoch': epoch_counter + 1,\n                            'arch': self.args.arch,\n                            'state_dict': self.model.state_dict(),\n                            'optimizer': self.optimizer.state_dict(),\n                            'best_top1': best_top1\n                        }, is_best=True, filename=best_ckpt_path)\n                        logging.info(f\"New best model saved with Top1={best_top1:.2f}% at epoch {epoch_counter+1}, step {n_iter}\")\n\n                    \n\n                n_iter += 1\n\n            # warmup for the first 10 epochs\n            if epoch_counter >= 10:\n                self.scheduler.step()\n            logging.debug(f\"Epoch: {epoch_counter}\\tLoss: {loss}\\tTop1 accuracy: {top1[0]}\")\n\n        logging.info(\"Training has finished.\")\n        # save model checkpoints\n        checkpoint_name = os.path.join(self.args.log_dir, f'checkpoint_{self.args.epochs:04d}.pth.tar')\n        \n        save_checkpoint({\n            'epoch': self.args.epochs,\n            'arch': self.args.arch,\n            'state_dict': self.model.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n        }, is_best=False, filename=checkpoint_name)\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exceptions","metadata":{}},{"cell_type":"code","source":"class BaseSimCLRException(Exception):\n    \"\"\"Base exception\"\"\"\n\n\nclass InvalidBackboneError(BaseSimCLRException):\n    \"\"\"Raised when the choice of backbone Convnet is invalid.\"\"\"\n\n\nclass InvalidDatasetSelection(BaseSimCLRException):\n    \"\"\"Raised when the choice of dataset is invalid.\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class ContrastiveLearningViewGenerator(object):\n    \"\"\"Take two random crops of one image as the query and key.\"\"\"\n\n    def __init__(self, base_transform, n_views=2):\n        self.base_transform = base_transform\n        self.n_views = n_views\n\n    def __call__(self, x):\n        return [self.base_transform(x) for i in range(self.n_views)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.transforms import transforms\nfrom torchvision import transforms, datasets\n\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\n\nclass ContrastiveLearningDataset:\n    def __init__(self, root_folder):\n        self.root_folder = root_folder\n\n    @staticmethod\n    def get_simclr_pipeline_transform(size, s=1):\n        \"\"\"Return a set of data augmentation transformations as described in the SimCLR paper.\"\"\"\n        color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n        data_transforms = transforms.Compose([transforms.RandomResizedCrop(size=size),\n                                              transforms.RandomHorizontalFlip(),\n                                              transforms.RandomApply([color_jitter], p=0.8),\n                                              transforms.RandomGrayscale(p=0.2),\n                                              GaussianBlur(kernel_size=int(0.1 * size)),\n                                              transforms.ToTensor()])\n        return data_transforms\n\n    def get_dataset(self, name, n_views):\n        valid_datasets = {\n                        'cifar10': lambda: datasets.CIFAR10(self.root_folder, train=True,\n                                                          transform=ContrastiveLearningViewGenerator(\n                                                              self.get_simclr_pipeline_transform(32),\n                                                              n_views),\n                                                          download=True),\n                        'stl10': lambda: datasets.STL10(self.root_folder, split='unlabeled',\n                                                      transform=ContrastiveLearningViewGenerator(\n                                                          self.get_simclr_pipeline_transform(96),\n                                                          n_views),\n                                                      download=True),\n                        'tinyimagenet' : lambda: ImageFolder(\n                            root=os.path.join(self.root_folder, 'tiny-imagenet-200', 'train'),\n                            transform=ContrastiveLearningViewGenerator(\n                                self.get_simclr_pipeline_transform(64),  \n                                n_views))\n                        }\n\n        try:\n            dataset_fn = valid_datasets[name]\n        except KeyError:\n            raise InvalidDatasetSelection()\n        else:\n            return dataset_fn()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Encoder Model","metadata":{}},{"cell_type":"code","source":"\nimport torchvision.models as models\n\n\n\nclass ResNetSimCLR(nn.Module):\n\n    def __init__(self, base_model, out_dim):\n        super(ResNetSimCLR, self).__init__()\n        self.resnet_dict = {\"resnet18\": models.resnet18(weights=None, num_classes=out_dim),\n                            \"resnet50\": models.resnet50(weights=None, num_classes=out_dim)}\n\n        self.backbone = self._get_basemodel(base_model)\n        dim_mlp = self.backbone.fc.in_features\n\n        # add mlp projection head\n        self.backbone.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.backbone.fc)\n\n    def _get_basemodel(self, model_name):\n        try:\n            model = self.resnet_dict[model_name]\n        except KeyError:\n            raise InvalidBackboneError(\n                \"Invalid backbone architecture. Check the config file and pass one of: resnet18 or resnet50\")\n        else:\n            return model\n\n    def forward(self, x):\n        return self.backbone(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"\nmodel_names = sorted(name for name in models.__dict__\n                     if name.islower() and not name.startswith(\"__\")\n                     and callable(models.__dict__[name]))\nprint(model_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from types import SimpleNamespace\nimport torch.backends.cudnn as cudnn\n\nargs = SimpleNamespace()\nargs.device = torch.device('cuda')\nargs.data = '/kaggle/input/tiny-imagenet/tiny-imagenet-200'\ncudnn.deterministic = True\ncudnn.benchmark = True\nargs.dataset_name = 'tinyimagenet'\nargs.n_views = 2\nargs.batch_size = 256\nargs.out_dim = 128\nargs.lr = 0.0003\nargs.weight_decay = 1e-4\nargs.arch = 'resnet18'\nargs.workers = 2\nargs.gpu_index = 0\nargs.log_dir = '/kaggle/working/logs/simclr'\nargs.fp16_precision = True\nargs.epochs = 10\nargs.temperature = 0.07\nargs.seed = 1\nargs.log_every_n_steps = 100\ndataset = ContrastiveLearningDataset(args.data)\n\ntrain_dataset = dataset.get_dataset(args.dataset_name, args.n_views)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=args.batch_size, shuffle=True,\n    num_workers=args.workers, pin_memory=True, drop_last=True)\n\nmodel = ResNetSimCLR(base_model=args.arch, out_dim=args.out_dim)\n\noptimizer = torch.optim.Adam(model.parameters(), args.lr, weight_decay=args.weight_decay)\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader), eta_min=0,\n                                                       last_epoch=-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.cuda.device(args.gpu_index):\n    simclr = SimCLR(model=model, optimizer=optimizer, scheduler=scheduler, args=args)\n    simclr.train(train_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}